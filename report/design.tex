\chapter{Design And Architecture\label{chap:design}}

This chapter describes the design of the system. This includes both the underlying decisions of the system as well as the user interface design. The chapter also presents on a more detailed view of the system's architecture.

\section{System Design}

The system has to work in two major steps. The first step is to take semi-structured natural language script and analyze it. This means that the system must analyze each dialogue line and infer some emotional values from them. Using those emotional values and the length of the dialogue line, the system must find best matching animation clips from the motion capture database. The results of this step is a file that specifies which characters say which lines of dialogue and it also specifies which animated clips accompany those dialogue lines.

After this step is completed, the file must be intepreted into a final scene. The importer must be able to read the file, import required models and animations and generate the final scene. As every animation software and game engine is a little different, each of them would need a custom importer. Those would be very similar in principle, but differ slightly because of the implementation of given software. For my project I have created the importer for Blender. This is because Blender is open source and available to anyone, as well as I am the most familiar with this software.


\section{Emotions}

For my project I have decided to only use the following emotions: joy, fear, disgust, anger and sadness. Tradinionally, surprise is also part of the six basic emotions model by Paul Ekman and Wallace V. Friesen. However, IBM Watson Tone Analyzer, that I am using for sentiment analysis, is unable to detect surprise in the text (more on that in section ~\ref{sec:emoanal}) - therefore I had to abstain from using this emotion in this project. The emotions are expressed as decimal numbers between 0 and 1 - this identifies whether a given emotion is present in a specific motion clip or text and how intense that emotion is. It also allows to create a mixture of emotions in order to represent more complex emotions.

\section{Architecture}

The architecture of the system is essentially a pipeline. The modules process resources and pass them onto the next module. They can be completely unaware of each other. This allows for a lot of flexibility and helps achieve some of the requirements. The emotion analysis API can be replaced with a different one, the user can use a custom motion capture database, and a different importer can be created if the user wishes to use software other than blender.


There are three main modules:
\begin{itemize}
\item Text Analysis
\item Animation Clip Matcher
\item Animation Generator
\end{itemize}

The Text Analysis module parses the text and analyses emotion using some API. It takes a script as an input and passess the parsed and analyzed text to the Animation Clip Matcher.

The Animation Clip Matcher uses the motion capture database to find the best animation clips to accompany the speech. The Animation Clip Matcher must take into account the emotion analysis of the text and find animations with similar emotional score. Another important constraint is the time of the animation. The animated clips have different lengths and a chosen clip must not be significantly longer than the spoken/read text. In case the animated clip is shorter, the Matcher must choose more than one matching clip. The Matcher outputs a JSON file which specifies all dialogue lines; it states which characters perform which emotional gesture actions and where is the animation file located.

The Animation Generator reads the JSON file and imports all needed animations. The user is now able to assign character model for each character and run the generator. The generator will assemble the animations in correct order, focus the camera on a currently speaking character and add subtitles. The output is an animated scene that can be either manually edited, exported to a file or rendered.

The full diagram of the system can be seen in figure ~\ref{fig:architecture}.

\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/architecture.png}}}
\caption{The pipeline architecture of the system}\label{fig:architecture}
\end{figure}

\section{Character Armature and Model}

TO DO

\section{EMBD Iimporter}

\section{Input}

The input of the system is as follows:

The input is a file that represents a dialogue. Each character that participates in the scene must ble clearly stated. Each dialogue line must have a dialog line must have a character clearly associated with it. Character names are specified after five tabs. Dialogue Lines are specified after three tabs. Each file must end with "ENDSCRIPT" with no indentation.

I used this format as this format is often used to represent movie scripts. One can find hundreds of scripts saved in this format on The Internet Movie Script Database (\url{www.imsdb.com}). The example input can be seen in figure ~\ref{fig:inputscript}.


\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/script.png}}}
\caption{An example of system's input}\label{fig:inputscript}
\end{figure}


\section{Emotion Analysis} 
\label{sec:emoanal}
As aforementioned, there are many ways to perform emotion analysis of the text. I have chosen to use IBM Watson Tone analyzer for this task as it offers a state-of-the-art service accessible for prototyping (a lot of tools offering high quality emotional analysis are bult for commercial purposes and not available without paying high fees).

The system takes each dialogue line and sends to IBM Watson Tone Analyzer for analysis. Watson's REST API is used to accomplish that. Watson returns all the emotions with values between 0 and 1 found in the text. Each dialogue line now has emotional values assigned to it.

\section{Database}

The motion capture database consists of two main parts: the files that contain the animated clips and a database that holds the metadata about the animations.

Most animation come from the Max Planck's Research Institute Emotional Body Motion Database. This database is a set of motion capture clips performed by actors. The clips are supposed to represent emotions in various settings. Most of these animations need minor changes in order to be compatible with my system (e.g. a lot of clips were recorded while the actor was sitting, so the leg keyframes must be removed to make the model stand). The animations are exported to FBX files (proprietary file format owned by Autodesk, widely supported by many different frameworks) one animated clip per file.

The database is implemented using SQLite. It is the perfect tool for this task as the database needs to be simple, relatively small and easily searchable. The animation metadata is held in a table that look like this FIGURE. The emotional values of each animated clip need to be manually adjusted. When all values are set to zero, it means that the clip carries no emotional impact (neutral). An example of a few database records can be seen in figure ~\ref{fig:db}.

\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/db.png}}}
\caption{Database of animation clips}\label{fig:db}
\end{figure}

The other purpose of the database is to store the information about the models. The important information about a model is its name, file location, camera offset and rotation. Because models may be of different shapes and sizes, camera positioning during dialogue will not be the same. The database allows to specify a desirable camera position relative to the character that will allow to fully capture the character at a good angle. Example of a few model database records can be seen in figure ~\ref{fig:dbmodel}.

\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/dbmodel.png}}}
\caption{Database of character models}\label{fig:dbmodel}
\end{figure}


\section{Matcher}

TO DO


\section{Animation Generator (Importer)}

As a forementioned, in this project I have used Blender to support the animation generator model. The generator is a Blender addon.  It can either be used as a script, or be installed as an addon to be fully and permanently available within Blender.

The generator first reads the JSON file prepared by previous modules and loads required animation clips from files. The user needs assign a 3D character model to each character participating in the dialogue. The required 3D character models are animated and the camera is positioned accordingly to the model's metadata stored in the database. The animated characters are positioned to directly face each other (the program currently supports only up to two characters in one scene). Each animated action is assigned to a corresponding character model and then pushed on a separate NLA strip (figure ~\ref{fig:nla}). The program keeps track of how many frames are being used so that newly added actions begin just when the previous actions have ended.

To setup some basic background and feel, a floor (plane) is added beneath the characters and two directional lights cast light down from above the characters.

The camera is added to the scene and positioned using the metadata retrievec from the database. At the start of each character's actions the camera is positioned and rotated to focus at the character performing an action.

Subtitles are added as Video Editor Sequence Strips spanning between frames that correspond to the animation (figure ~\ref{fig:substrips}).

\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/nla.png}}}
\caption{NLA strips of the final animation}\label{fig:nla}
\end{figure}
\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/substrips.png}}}
\caption{Video strips featuring subtitles}\label{fig:substrips}
\end{figure}

The final animation can be edited and adjusted in Blender (figure ~\ref{fig:finalblend}), rendered (figure ~\ref{fig:finalrend}) or exported to file.


\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/finalblend.png}}}
\caption{Editing the final animation in Blender}\label{fig:finalblend}
\end{figure}
\begin{figure}[!ht]
\centerline{\fbox{\includegraphics[width = 30em]{img/finalrend.png}}}
\caption{Rendered final animation}\label{fig:finalrend}
\end{figure}

\section{User Interface}

The Animation Generator module is the only module that features a Graphical UI. The module is embedded into Blender and uses extends Blender's interface. 

The UI is minimalistic and simple to use. Upon installation and activation a new tab is added to the menu on the left hand side. Figure ~\ref {fig:ui_main}. Rectangle 1 represents the left hand side menu. Number 2 shows the tabs where the bottommost one belongs to the extension. Number 3 shows the actual UI of the extension.

\begin{figure}[!ht]
\centerline{\includegraphics[width = 18em]{img/ui_main.png}}
\caption{The addon UI}\label{fig:ui_main}
\end{figure}

From here the user is able to either clean the scene (might be useful especially if something unexpected happens or the work of the extension is interrupted) or prepare the scene. To prepare the scene the user must first initialize the four variables. The user needs to point the program to where the animations and models are stored as well as the database file and the scene file (JSON file generated by previous modules).

Upon pressing `Prepare` the program will prepare data for generating a scene. Required animations will be imported and the scene file will be parsed. There is one more step left to finalize the animation. When preparation is finished, a new menu will pop up below the currently existing one. The menu can be seen in figure ~\ref{fig:ui_finalize}. In the menu the user is required to assign a character model to each character existing in the scene. Upon pressing `Finalize` the full scene will be generated complete with lighting, camera movement and subtitles.

\begin{figure}[!ht]
\centerline{\includegraphics[width = 18em]{img/ui_finalize.png}}
\caption{The finalization step (new menu highlighted by white rectangle)}\label{fig:ui_finalize}
\end{figure}

They key aspect of the interface is its simplicity as it allows users to generate relatively complicated animation sequences by essentially using two buttons, with no knowledge about animation and little Blender expertise required. The UI also provides flexibility as the user can decide upon which character models to use.





